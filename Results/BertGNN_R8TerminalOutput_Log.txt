Last login: Wed Nov 30 22:47:06 on ttys000
(base) saisirishavadapalli@Sais-MacBook-Air ~ % cd desktop
(base) saisirishavadapalli@Sais-MacBook-Air desktop % cd BertGNN
(base) saisirishavadapalli@Sais-MacBook-Air BertGNN % python rain_bert_plus_gnn.py --dataset  R8 -m 0.7
python: can't open file '/Users/saisirishavadapalli/Desktop/BertGNN/rain_bert_plus_gnn.py': [Errno 2] No such file or directory
(base) saisirishavadapalli@Sais-MacBook-Air BertGNN % python train_bert_plus_gnn.py --dataset  R8 -m 0.7
arguments:
Namespace(max_length=128, batch_size=64, m=0.7, nb_epochs=50, bert_init='roberta-base', pretrained_bert_ckpt=None, dataset='R8', checkpoint_dir=None, gnn_model='gnn', gnn_layers=2, n_hidden=200, heads=8, dropout=0.5, gnn_lr=0.001, bert_lr=1e-05)
checkpoints will be saved in ./checkpoint/roberta-base_gnn_R8
(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
graph information:
Graph(num_nodes=15362, num_edges=3504462,
      ndata_schemes={'input_ids': Scheme(shape=(128,), dtype=torch.int64), 'attention_mask': Scheme(shape=(128,), dtype=torch.int64), 'label': Scheme(shape=(), dtype=torch.int64), 'train': Scheme(shape=(), dtype=torch.float32), 'val': Scheme(shape=(), dtype=torch.float32), 'test': Scheme(shape=(), dtype=torch.float32), 'label_train': Scheme(shape=(), dtype=torch.int64), 'cls_feats': Scheme(shape=(768,), dtype=torch.float32)}
      edata_schemes={'edge_weight': Scheme(shape=(), dtype=torch.float32)})
-----------------------------Log-Train Step Done---------------------------------------------------------
Epoch: 1  Train acc: 0.9409 loss: 0.1651  Val acc: 0.9361 loss: 0.1884  Test acc: 0.9466 loss: 0.1530
New checkpoint
-------------loading results--------------------------------
-----------------------------Log-Train Step Done---------------------------------------------------------
Epoch: 2  Train acc: 0.9457 loss: 0.1208  Val acc: 0.9416 loss: 0.1424  Test acc: 0.9616 loss: 0.0938
New checkpoint
-------------loading results--------------------------------
-----------------------------Log-Train Step Done---------------------------------------------------------
Epoch: 3  Train acc: 0.9692 loss: 0.0848  Val acc: 0.9599 loss: 0.1190  Test acc: 0.9644 loss: 0.0976
New checkpoint
-------------loading results--------------------------------
-----------------------------Log-Train Step Done---------------------------------------------------------
Epoch: 4  Train acc: 0.9714 loss: 0.0630  Val acc: 0.9708 loss: 0.0855  Test acc: 0.9808 loss: 0.0627
New checkpoint
-------------loading results--------------------------------
-----------------------------Log-Train Step Done---------------------------------------------------------
Epoch: 5  Train acc: 0.9674 loss: 0.0623  Val acc: 0.9580 loss: 0.0859  Test acc: 0.9735 loss: 0.0701
-----------------------------Log-Train Step Done---------------------------------------------------------
Epoch: 6  Train acc: 0.9840 loss: 0.0410  Val acc: 0.9836 loss: 0.0601  Test acc: 0.9817 loss: 0.0606
New checkpoint
-------------loading results--------------------------------
-----------------------------Log-Train Step Done---------------------------------------------------------
Epoch: 7  Train acc: 0.9791 loss: 0.0463  Val acc: 0.9726 loss: 0.0765  Test acc: 0.9817 loss: 0.0582
